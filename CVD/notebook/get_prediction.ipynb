{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544ea01c-88fb-4314-a0ff-53fdec0ca12f",
   "metadata": {},
   "source": [
    "# Load models and get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2dce663-5240-4a29-9695-f566424ea238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define the path to the model and scaler\n",
    "MODEL_PATH = \"models/computer_cnn_model.keras\"\n",
    "SCALER_PATH = \"models/scaler.pkl\"\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4925caaa-85ee-42c8-b093-51f5634d10e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load the scaler\n",
    "with open(SCALER_PATH, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"Scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fe3a3ca-9210-4f66-af74-42880a6d9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input data\n",
    "def preprocess_input(input_data):\n",
    "    \"\"\"\n",
    "    Preprocess input data for the CNN model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    Returns: preprocessed data with shape (n_samples, 13, 1)\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it isn't already\n",
    "    input_data = np.array(input_data)\n",
    "    \n",
    "    # Ensure input has the correct shape\n",
    "    if input_data.ndim == 1:\n",
    "        input_data = input_data.reshape(1, -1)  # Reshape single sample to (1, 13)\n",
    "    \n",
    "    # Check if input has 13 features\n",
    "    if input_data.shape[1] != 13:\n",
    "        raise ValueError(f\"Expected 13 features, got {input_data.shape[1]}\")\n",
    "    \n",
    "    # Scale the input data using the loaded scaler\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Reshape for CNN input: (n_samples, 13, 1)\n",
    "    input_reshaped = input_scaled.reshape(input_scaled.shape[0], input_scaled.shape[1], 1)\n",
    "    \n",
    "    return input_reshaped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2a8d355-c3a4-4f48-abb8-14ba3d690d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions\n",
    "def predict_heart_disease(input_data):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded CNN model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    Returns: predicted probabilities and binary predictions\n",
    "    \"\"\"\n",
    "    # Preprocess the input\n",
    "    input_processed = preprocess_input(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    probabilities = model.predict(input_processed).flatten()  # Probabilities for class 1\n",
    "    predictions = (probabilities > 0.5).astype(int)  # Binary predictions (0 or 1)\n",
    "    \n",
    "    return probabilities, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1946e8e-ab29-4fdb-8bf7-39d4d03e9748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step\n",
      "\n",
      "Prediction Results:\n",
      "Sample 1:\n",
      "  Probability of Heart Disease: 0.9999\n",
      "  Predicted Class: Heart Disease\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Example input: a single sample with 13 features\n",
    "    # Replace with actual data (values should correspond to the feature order in the training dataset)\n",
    "    example_input = [45, 0, 1, 130, 234, 0, 0, 175, 0, 0.6, 1, 0, 2]\n",
    "\n",
    "try:\n",
    "    probs, preds = predict_heart_disease(example_input)\n",
    "    \n",
    "    # Print results without loop\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(f\"Sample 1:\")\n",
    "    print(f\"  Probability of Heart Disease: {probs[0]:.4f}\")\n",
    "    print(f\"  Predicted Class: {'Heart Disease' if preds[0] == 1 else 'No Heart Disease'}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb85cd-909d-47c5-9138-0c582d3c3af3",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ccca88ef-ca0c-4733-a3aa-a67bed48c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5308f15-9d80-4ae6-a4db-a35fcad6fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "147be63d-6a1e-405c-bd47-354181106be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Scaler loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the model and scaler\n",
    "MODEL_PATH = \"models/computer_cnn_model.keras\"\n",
    "SCALER_PATH = \"models/scaler.pkl\"\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = tf.keras.models.load_model(MODEL_PATH)\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "# Load the scaler\n",
    "with open(SCALER_PATH, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"Scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b74d1386-659a-4ad9-aa97-3b7905a52776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input data\n",
    "def preprocess_input(input_data):\n",
    "    \"\"\"\n",
    "    Preprocess input data for the CNN model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    Returns: preprocessed data with shape (n_samples, 13, 1)\n",
    "    \"\"\"\n",
    "    # Convert input to numpy array if it isn't already\n",
    "    input_data = np.array(input_data)\n",
    "    \n",
    "    # Ensure input has the correct shape\n",
    "    if input_data.ndim == 1:\n",
    "        input_data = input_data.reshape(1, -1)  # Reshape single sample to (1, 13)\n",
    "    \n",
    "    # Check if input has 13 features\n",
    "    if input_data.shape[1] != 13:\n",
    "        raise ValueError(f\"Expected 13 features, got {input_data.shape[1]}\")\n",
    "    \n",
    "    # Scale the input data using the loaded scaler\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Reshape for CNN input: (n_samples, 13, 1)\n",
    "    input_reshaped = input_scaled.reshape(input_scaled.shape[0], input_scaled.shape[1], 1)\n",
    "    \n",
    "    return input_reshaped\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_heart_disease(input_data):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded CNN model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    Returns: predicted probabilities and binary predictions\n",
    "    \"\"\"\n",
    "    # Preprocess the input\n",
    "    input_processed = preprocess_input(input_data)\n",
    "    \n",
    "    # Make predictions\n",
    "    probabilities = model.predict(input_processed, verbose=0).flatten()  # Probabilities for class 1\n",
    "    predictions = (probabilities > 0.5).astype(int)  # Binary predictions (0 or 1)\n",
    "    \n",
    "    return probabilities, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa2fcc9-164c-43b5-820e-21acd6a81879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint to handle predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get JSON data from the request\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Extract input parameters (expecting a single sample as a list or dict)\n",
    "        if isinstance(data, dict):\n",
    "            # Convert dict to list in the correct order\n",
    "            expected_keys = [\n",
    "                'age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol',\n",
    "                'fasting_blood_sugar', 'resting_electrocardiogram', 'max_heart_rate_achieved',\n",
    "                'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels',\n",
    "                'thalassemia'\n",
    "            ]\n",
    "            if not all(key in data for key in expected_keys):\n",
    "                return jsonify({'error': 'Missing required parameters'}), 400\n",
    "            input_data = [float(data[key]) for key in expected_keys]\n",
    "        elif isinstance(data, list):\n",
    "            # Assume list is in the correct order\n",
    "            input_data = data\n",
    "        else:\n",
    "            return jsonify({'error': 'Invalid input format. Expecting a list or dict.'}), 400\n",
    "        \n",
    "        # Validate input length\n",
    "        if len(input_data) != 13:\n",
    "            return jsonify({'error': f'Expected 13 features, got {len(input_data)}'}), 400\n",
    "        \n",
    "        # Make predictions\n",
    "        probs, preds = predict_heart_disease(input_data)\n",
    "        \n",
    "        # Prepare response\n",
    "        result = {\n",
    "            'probability': float(probs[0]),  # Convert to float for JSON serialization\n",
    "            'prediction': 'Heart Disease' if preds[0] == 1 else 'No Heart Disease'\n",
    "        }\n",
    "        \n",
    "        return jsonify(result), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b38617f0-fba6-4ec7-b432-175da4c93cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.103:5000\n",
      "Press CTRL+C to quit\n",
      " * Restarting with stat\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\u001b[31m:\u001b[39m 1\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1779ee81-e3dd-44e5-98ec-2836af866cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
