{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "544ea01c-88fb-4314-a0ff-53fdec0ca12f",
   "metadata": {},
   "source": [
    "# Load models and get predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cb85cd-909d-47c5-9138-0c582d3c3af3",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "-----------------------------------------------------\n",
    "-----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccca88ef-ca0c-4733-a3aa-a67bed48c7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5308f15-9d80-4ae6-a4db-a35fcad6fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147be63d-6a1e-405c-bd47-354181106be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path to the model and scaler\n",
    "# MODEL_PATH = \"models/computer_cnn_model.keras\"\n",
    "# SCALER_PATH = \"models/scaler.pkl\"\n",
    "\n",
    "# # Load the trained CNN model\n",
    "# model = tf.keras.models.load_model(MODEL_PATH)\n",
    "# print(\"Model loaded successfully!\")\n",
    "\n",
    "# # Load the scaler\n",
    "# with open(SCALER_PATH, 'rb') as f:\n",
    "#     scaler = pickle.load(f)\n",
    "# print(\"Scaler loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b74d1386-659a-4ad9-aa97-3b7905a52776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to preprocess input data\n",
    "# def preprocess_input(input_data):\n",
    "#     \"\"\"\n",
    "#     Preprocess input data for the CNN model.\n",
    "#     input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "#     Returns: preprocessed data with shape (n_samples, 13, 1)\n",
    "#     \"\"\"\n",
    "#     # Convert input to numpy array if it isn't already\n",
    "#     input_data = np.array(input_data)\n",
    "    \n",
    "#     # Ensure input has the correct shape\n",
    "#     if input_data.ndim == 1:\n",
    "#         input_data = input_data.reshape(1, -1)  # Reshape single sample to (1, 13)\n",
    "    \n",
    "#     # Check if input has 13 features\n",
    "#     if input_data.shape[1] != 13:\n",
    "#         raise ValueError(f\"Expected 13 features, got {input_data.shape[1]}\")\n",
    "    \n",
    "#     # Scale the input data using the loaded scaler\n",
    "#     input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "#     # Reshape for CNN input: (n_samples, 13, 1)\n",
    "#     input_reshaped = input_scaled.reshape(input_scaled.shape[0], input_scaled.shape[1], 1)\n",
    "    \n",
    "#     return input_reshaped\n",
    "\n",
    "# # Function to make predictions\n",
    "# def predict_heart_disease(input_data):\n",
    "#     \"\"\"\n",
    "#     Make predictions using the loaded CNN model.\n",
    "#     input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "#     Returns: predicted probabilities and binary predictions\n",
    "#     \"\"\"\n",
    "#     # Preprocess the input\n",
    "#     input_processed = preprocess_input(input_data)\n",
    "    \n",
    "#     # Make predictions\n",
    "#     probabilities = model.predict(input_processed, verbose=0).flatten()  # Probabilities for class 1\n",
    "#     predictions = (probabilities > 0.5).astype(int)  # Binary predictions (0 or 1)\n",
    "    \n",
    "#     return probabilities, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfa2fcc9-164c-43b5-820e-21acd6a81879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # API endpoint to handle predictions\n",
    "# @app.route('/predict', methods=['POST'])\n",
    "# def predict():\n",
    "#     try:\n",
    "#         # Get JSON data from the request\n",
    "#         data = request.get_json()\n",
    "        \n",
    "#         # Extract input parameters (expecting a single sample as a list or dict)\n",
    "#         if isinstance(data, dict):\n",
    "#             # Convert dict to list in the correct order\n",
    "#             expected_keys = [\n",
    "#                 'age', 'sex', 'chest_pain_type', 'resting_blood_pressure', 'cholesterol',\n",
    "#                 'fasting_blood_sugar', 'resting_electrocardiogram', 'max_heart_rate_achieved',\n",
    "#                 'exercise_induced_angina', 'st_depression', 'st_slope', 'num_major_vessels',\n",
    "#                 'thalassemia'\n",
    "#             ]\n",
    "#             if not all(key in data for key in expected_keys):\n",
    "#                 return jsonify({'error': 'Missing required parameters'}), 400\n",
    "#             input_data = [float(data[key]) for key in expected_keys]\n",
    "#         elif isinstance(data, list):\n",
    "#             # Assume list is in the correct order\n",
    "#             input_data = data\n",
    "#         else:\n",
    "#             return jsonify({'error': 'Invalid input format. Expecting a list or dict.'}), 400\n",
    "        \n",
    "#         # Validate input length\n",
    "#         if len(input_data) != 13:\n",
    "#             return jsonify({'error': f'Expected 13 features, got {len(input_data)}'}), 400\n",
    "        \n",
    "#         # Make predictions\n",
    "#         probs, preds = predict_heart_disease(input_data)\n",
    "        \n",
    "#         # Prepare response\n",
    "#         result = {\n",
    "#             'probability': float(probs[0]),  # Convert to float for JSON serialization\n",
    "#             'prediction': 'Heart Disease' if preds[0] == 1 else 'No Heart Disease'\n",
    "#         }\n",
    "        \n",
    "#         return jsonify(result), 200\n",
    "    \n",
    "#     except Exception as e:\n",
    "#         return jsonify({'error': str(e)}), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b38617f0-fba6-4ec7-b432-175da4c93cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1779ee81-e3dd-44e5-98ec-2836af866cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaler loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define paths to models and scaler\n",
    "MODEL_PATHS = {\n",
    "    'CNN': 'models/computer_cnn_model.keras',\n",
    "    'Transformer': 'models/kaggle_transformer_model.keras',\n",
    "    'LGBMClassifier': 'models/lgbm_model.pkl',\n",
    "    'Tuned Transformer': 'models/kaggle_best_cnn_tuned_model.keras'\n",
    "}\n",
    "SCALER_PATH = 'models/scaler.pkl'\n",
    "\n",
    "# Load scaler\n",
    "with open(SCALER_PATH, 'rb') as f:\n",
    "    scaler = pickle.load(f)\n",
    "print(\"Scaler loaded successfully!\")\n",
    "\n",
    "# Load models (lazy loading to avoid memory issues)\n",
    "models = {}\n",
    "def load_model(model_name):\n",
    "    if model_name not in models:\n",
    "        if model_name == 'LGBMClassifier':\n",
    "            with open(MODEL_PATHS[model_name], 'rb') as f:\n",
    "                models[model_name] = pickle.load(f)\n",
    "            print(f\"{model_name} loaded successfully!\")\n",
    "        else:\n",
    "            models[model_name] = tf.keras.models.load_model(MODEL_PATHS[model_name])\n",
    "            print(f\"{model_name} loaded successfully!\")\n",
    "    return models[model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0dbd868f-16b0-44a9-8e57-c9b0688109d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess input data\n",
    "def preprocess_input(input_data, model_name):\n",
    "    \"\"\"\n",
    "    Preprocess input data for the specified model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    Returns: preprocessed data suitable for the model\n",
    "    \"\"\"\n",
    "    input_data = np.array(input_data)\n",
    "    if input_data.ndim == 1:\n",
    "        input_data = input_data.reshape(1, -1)\n",
    "    if input_data.shape[1] != 13:\n",
    "        raise ValueError(f\"Expected 13 features, got {input_data.shape[1]}\")\n",
    "    \n",
    "    # Scale the input data\n",
    "    input_scaled = scaler.transform(input_data)\n",
    "    \n",
    "    # Reshape for CNN/Transformer models (n_samples, 13, 1)\n",
    "    if model_name in ['CNN', 'Transformer', 'Tuned Transformer']:\n",
    "        input_reshaped = input_scaled.reshape(input_scaled.shape[0], input_scaled.shape[1], 1)\n",
    "        return input_reshaped\n",
    "    # LGBMClassifier expects 2D input (n_samples, 13)\n",
    "    return input_scaled\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_heart_disease(input_data, model_name):\n",
    "    \"\"\"\n",
    "    Make predictions using the specified model.\n",
    "    input_data: numpy array or list with shape (n_samples, 13) or a single sample (13,)\n",
    "    model_name: str, one of 'CNN', 'Transformer', 'LGBMClassifier', 'Tuned Transformer'\n",
    "    Returns: predicted probabilities and binary predictions\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = load_model(model_name)\n",
    "    \n",
    "    # Preprocess the input\n",
    "    input_processed = preprocess_input(input_data, model_name)\n",
    "    \n",
    "    # Make predictions\n",
    "    if model_name == 'LGBMClassifier':\n",
    "        probabilities = model.predict_proba(input_processed)[:, 1]  # Probability of class 1\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "    else:\n",
    "        probabilities = model.predict(input_processed, verbose=0).flatten()\n",
    "        predictions = (probabilities > 0.5).astype(int)\n",
    "    \n",
    "    return probabilities, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c0e406-6942-45cf-ab9e-ac639dde0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# API endpoint to handle predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Get JSON data from the request\n",
    "        data = request.get_json()\n",
    "        \n",
    "        # Validate input and model\n",
    "        if not data or 'input' not in data or 'model' not in data:\n",
    "            return jsonify({'error': 'Missing input or model parameter'}), 400\n",
    "        \n",
    "        input_data = data['input']\n",
    "        model_name = data['model']\n",
    "        \n",
    "        if model_name not in MODEL_PATHS:\n",
    "            return jsonify({'error': f\"Invalid model. Choose from {list(MODEL_PATHS.keys())}\"}), 400\n",
    "        \n",
    "        if not isinstance(input_data, list) or len(input_data) != 13:\n",
    "            return jsonify({'error': 'Input must be a list with 13 features'}), 400\n",
    "        \n",
    "        # Make predictions\n",
    "        probs, preds = predict_heart_disease(input_data, model_name)\n",
    "        \n",
    "        # Prepare response\n",
    "        result = {\n",
    "            'probability': float(probs[0]),\n",
    "            'prediction': 'Heart Disease' if preds[0] == 1 else 'No Heart Disease',\n",
    "            'model': model_name\n",
    "        }\n",
    "        \n",
    "        return jsonify(result), 200\n",
    "    \n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dc7ab1-61c9-4cfa-a760-de6d86515762",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=5000, debug=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
